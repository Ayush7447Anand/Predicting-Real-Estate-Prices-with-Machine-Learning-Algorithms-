# -*- coding: utf-8 -*-
"""Real Estate Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CHDUAhnn9p1SBVJFus9iowvYjBQpDGtw

# **Real Estate Project Price Predictor**
"""

import pandas as pd
housing = pd.read_csv('/content/housing_data.csv')
#l=list(housing)
housing.head()
l=len(housing)
print(l)

housing.info()

housing['CHAS'].value_counts()

housing.describe()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import matplotlib.pyplot as plt
housing.hist(bins=50, figsize=(20,15))

"""## **SPLITING TRAINING AND TEST DATASET**"""

# import numpy as np
# def split_train_test(data, test_ratio):
#   np.random.seed(42)
#   shuffled=np.random.permutation(len(data))
#   print(shuffled)
#   test_set_size=int(len(data)* test_ratio)
#   test_indices=shuffled[:test_set_size]
#   train_indices=shuffled[test_set_size:]
#   return data.iloc[train_indices], data.iloc[test_indices]

# train_set, test_set= split_train_test(housing, 0.2)
# print(f'Rows in training set= {len(train_set)}\nRows in test set= {len(test_set)}\n')

from sklearn.model_selection import train_test_split
train_set, test_set= train_test_split(housing, test_size=0.2, random_state=42)
print(f'Rows in training set= {len(train_set)}\nRows in test set= {len(test_set)}\n')

from sklearn.model_selection import StratifiedShuffleSplit
split= StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing['CHAS']):
  strat_train_set= housing.iloc[train_index]
  strat_test_set= housing.iloc[test_index]

strat_test_set.describe()

strat_test_set.info()

strat_train_set['CHAS'].value_counts()

strat_test_set['CHAS'].value_counts()

housing= strat_train_set.copy()

"""## **LOOKING FOR CO-RELATIONS B/W FEATURES**"""

corr_matrix= housing.corr()
corr_matrix['MEDV'].sort_values(ascending=False)

from pandas.plotting import scatter_matrix
attributes=['MEDV','RM','ZN','LSTAT']
scatter_matrix(housing[attributes], figsize=(12,8))

housing.plot(kind='scatter', x='RM',y='MEDV',alpha=0.8)

"""## **Trying out attribute combinations**"""

housing['TAXRM']= housing['TAX']/housing['RM']
housing.head()

corr_matrix= housing.corr()
corr_matrix['MEDV'].sort_values(ascending= False)

housing.plot(kind= 'scatter', x='TAXRM', y='MEDV', alpha=0.6)

housing.describe()

housing = strat_train_set.drop('MEDV', axis=1)
housing_labels = strat_train_set['MEDV'].copy()

"""## **Missing Data Attribute**"""

median=housing['RM'].median()
housing['RM'].fillna(median)

housing.describe()  #before we started imputing

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='median')
imputer.fit(housing)
imputer.statistics_

x= imputer.transform(housing)
housing_tr = pd.DataFrame(x, columns= housing.columns)
housing_tr.describe()

"""## **Scikit-learn Design**"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
my_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy = 'median')),
    ('std_scaler', StandardScaler()),
])

housing_num_tr = my_pipeline.fit_transform(housing_tr)
housing_num_tr.shape

"""## **Selecting a desired model for Real Estates**

## **1. LINEAR REGRESSION**
"""

from sklearn.linear_model import LinearRegression
model1 = LinearRegression()
model1.fit(housing_num_tr, housing_labels)

some_data1 = housing.iloc[:5]
some_labels1 = housing_labels.iloc[:5]
prepared_data1  = my_pipeline.transform(some_data1)
model1.predict(prepared_data1)

list(some_labels1)

"""## **Evaluating the Model**"""

import numpy as np
from sklearn.metrics import mean_squared_error
housing_predictions1 = model1.predict(housing_num_tr)
lin_mse1 = mean_squared_error(housing_labels, housing_predictions1)
lin_rmse1 = np.sqrt(lin_mse1)
lin_mse1

"""## **Cross Validation**"""

from sklearn.model_selection import cross_val_score
scores1 = cross_val_score(model1, housing_num_tr, housing_labels, scoring='neg_mean_squared_error', cv=10)
rmse_scores1 = np.sqrt(-scores1)
rmse_scores1

def print_scores(scores):
  print('Scores: ', scores)
  print('Mean: ', scores.mean())
  print('Standard deviation: ', scores.std())

print_scores(rmse_scores1)

"""**Saving the model**"""

from joblib import dump, load
dump(model1, 'Linear_Regression_Model.joblib')

"""**Tesing the data**"""

X_test = strat_test_set.drop('MEDV', axis=1)
Y_test = strat_test_set['MEDV'].copy()
X_test_prepared = my_pipeline.transform(X_test)
final_predictions = model1.predict(X_test_prepared)
final_mse1 = mean_squared_error(Y_test, final_predictions)
final_rmse1 = np.sqrt(final_mse1)
print(final_predictions, list(Y_test))

final_mse1

from joblib import dump, load
import numpy as np
model = load('Dragon1.joblib')

features = np.array([[-0.43942006,  3.12628155, -1.12165014, -0.27288841, -1.42262747,
       -0.23979304, -1.31238772,  2.61111401, -1.0016859 , -0.5778192 ,
       -0.97491834,  0.41164221, -0.86091034]])

m1= model.predict(features)
print(m1)

"""## **DECISION TREE**"""

from sklearn.tree import DecisionTreeRegressor
model2 = DecisionTreeRegressor()
model2.fit(housing_num_tr, housing_labels)

some_data2 = housing.iloc[:5]
some_labels2 = housing_labels.iloc[:5]
prepared_data2  = my_pipeline.transform(some_data2)
model2.predict(prepared_data2)

list(some_labels2)

import numpy as np
from sklearn.metrics import mean_squared_error
housing_predictions2 = model2.predict(housing_num_tr)
mse2 = mean_squared_error(housing_labels, housing_predictions2)
rmse2 = np.sqrt(mse2)
mse2

"""## **Using better evaluation technique - CROSS VALIDATION**"""

from sklearn.model_selection import cross_val_score
scores2 = cross_val_score(model2, housing_num_tr, housing_labels, scoring='neg_mean_squared_error', cv=10)
rmse_scores2 = np.sqrt(-scores2)
rmse_scores2

print_scores(rmse_scores2)

"""Save the model"""

from joblib import dump, load
dump(model2, 'Dragon2.joblib')

"""**Testing the data set**"""

X_test = strat_test_set.drop('MEDV', axis=1)
Y_test = strat_test_set['MEDV'].copy()
X_test_prepared2 = my_pipeline.transform(X_test)
final_predictions2 = model2.predict(X_test_prepared2)
final_mse2 = mean_squared_error(Y_test, final_predictions2)
final_rmse2 = np.sqrt(final_mse2)
print(final_predictions2, list(Y_test))

final_mse2

"""**Prediction of the Model**"""

from joblib import dump, load
import numpy as np
model = load('Dragon2.joblib')

features = np.array([[-0.43942006,  3.12628155, -1.12165014, -0.27288841, -1.42262747,
       -0.23979304, -1.31238772,  2.61111401, -1.0016859 , -0.5778192 ,
       -0.97491834,  0.41164221, -0.86091034]])

m2=model.predict(features)
print(m2)

"""## **RANDOM FOREST**"""

from sklearn.ensemble import RandomForestRegressor
model3 = RandomForestRegressor()
model3.fit(housing_num_tr, housing_labels)

some_data3 = housing.iloc[:5]
some_labels3 = housing_labels.iloc[:5]
prepared_data3 = my_pipeline.transform(some_data3)
model3.predict(prepared_data3)

list(some_labels3)

import numpy as np
from sklearn.metrics import mean_squared_error
housing_predictions3 = model3.predict(housing_num_tr)
mse3 = mean_squared_error(housing_labels, housing_predictions3)
rmse3 = np.sqrt(mse3)
mse3

from sklearn.model_selection import cross_val_score
scores3 = cross_val_score(model3, housing_num_tr, housing_labels, scoring='neg_mean_squared_error', cv=10)
rmse_scores3 = np.sqrt(-scores3)
rmse_scores3

print_scores(rmse_scores3)

"""## **Saving the Model**"""

from joblib import dump, load
dump(model3, 'Dragon3.joblib')

"""# **Testing the model on test data**"""

X_test = strat_test_set.drop('MEDV', axis=1)
Y_test = strat_test_set['MEDV'].copy()
X_test_prepared3 = my_pipeline.transform(X_test)
final_predictions3 = model3.predict(X_test_prepared3)
final_mse3 = mean_squared_error(Y_test, final_predictions3)
final_rmse3 = np.sqrt(final_mse3)
print(final_predictions3, list(Y_test))

final_mse3

"""## **Using the Model**"""

from joblib import dump, load
import numpy as np
model = load('Dragon3.joblib')

features = np.array([[-0.43942006,  3.12628155, -1.12165014, -0.27288841, -1.42262747,
       -0.23979304, -1.31238772,  2.61111401, -1.0016859 , -0.5778192 ,
       -0.97491834,  0.41164221, -0.86091034]])

m3 = model.predict(features)
m3

"""# **Comparation between the 3 methods**"""

print(f''' LINEAR REGRESSION
              Model prediction- {m1}
              mean square error = {final_mse1} ''')
print_scores(rmse_scores1)
print()
print(f''' DECISION TREE
              Model prediction- {m2}
              mean square error = {final_mse2} ''')
print_scores(rmse_scores2)
print()
print(f''' LINEAR REGRESSION
              Model prediction- {m3}
              mean square error = {final_mse3} ''')
print_scores(rmse_scores3)

"""## **SUPPORT VECTOR MACHINES (SVM)**"""

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from joblib import dump, load
import numpy as np

# Create and fit the SVR model
model_svm = SVR(kernel='linear')
model_svm.fit(housing_num_tr, housing_labels)

# Prepare test data
X_test = strat_test_set.drop('MEDV', axis=1)
Y_test = strat_test_set['MEDV'].copy()
X_test_prepared = my_pipeline.transform(X_test)

# Make predictions
final_predictions_svm = model_svm.predict(X_test_prepared)

# Calculate RMSE for SVM
final_mse_svm = mean_squared_error(Y_test, final_predictions_svm)
final_rmse_svm = np.sqrt(final_mse_svm)
print("Final RMSE using SVM:", final_rmse_svm)

# Save the SVM model
dump(model_svm, 'SVM_Model.joblib')

# Load the SVM model
model_svm_loaded = load('SVM_Model.joblib')

# Example feature array
features_svm = np.array([[-0.43942006,  3.12628155, -1.12165014, -0.27288841, -1.42262747,
                         -0.23979304, -1.31238772,  2.61111401, -1.0016859 , -0.5778192 ,
                         -0.97491834,  0.41164221, -0.86091034]])

# Predict using the loaded SVM model
m1_svm = model_svm_loaded.predict(features_svm)
print("Predicted Price using SVM:", m1_svm)

"""## **GRADIENT BOOST**"""

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from joblib import dump, load
import numpy as np

# Create and fit the Gradient Boosting model
model_gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
model_gb.fit(housing_num_tr, housing_labels)

# Prepare test data
X_test = strat_test_set.drop('MEDV', axis=1)
Y_test = strat_test_set['MEDV'].copy()
X_test_prepared = my_pipeline.transform(X_test)

# Make predictions
final_predictions_gb = model_gb.predict(X_test_prepared)

# Calculate RMSE for Gradient Boosting
final_mse_gb = mean_squared_error(Y_test, final_predictions_gb)
final_rmse_gb = np.sqrt(final_mse_gb)
print("Final RMSE using Gradient Boosting:", final_rmse_gb)

# Save the Gradient Boosting model
dump(model_gb, 'GradientBoosting_Model.joblib')

# Load the Gradient Boosting model
model_gb_loaded = load('GradientBoosting_Model.joblib')

# Example feature array
features_gb = np.array([[-0.43942006,  3.12628155, -1.12165014, -0.27288841, -1.42262747,
                         -0.23979304, -1.31238772,  2.61111401, -1.0016859 , -0.5778192 ,
                         -0.97491834,  0.41164221, -0.86091034]])

# Predict using the loaded Gradient Boosting model
m1_gb = model_gb_loaded.predict(features_gb)
print("Predicted Price using Gradient Boosting:", m1_gb)

"""# **K-NEAREST NEIGHBOUR**"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from joblib import dump, load
import numpy as np

# Create and fit the K-Nearest Neighbors model
model_knn = KNeighborsRegressor(n_neighbors=5)
model_knn.fit(housing_num_tr, housing_labels)

# Prepare test data
X_test = strat_test_set.drop('MEDV', axis=1)
Y_test = strat_test_set['MEDV'].copy()
X_test_prepared = my_pipeline.transform(X_test)

# Make predictions
final_predictions_knn = model_knn.predict(X_test_prepared)

# Calculate RMSE for K-Nearest Neighbors
final_mse_knn = mean_squared_error(Y_test, final_predictions_knn)
final_rmse_knn = np.sqrt(final_mse_knn)
print("Final RMSE using K-Nearest Neighbors:", final_rmse_knn)

# Save the K-Nearest Neighbors model
dump(model_knn, 'KNN_Model.joblib')

# Load the K-Nearest Neighbors model
model_knn_loaded = load('KNN_Model.joblib')

# Example feature array
features_knn = np.array([[-0.43942006,  3.12628155, -1.12165014, -0.27288841, -1.42262747,
                          -0.23979304, -1.31238772,  2.61111401, -1.0016859 , -0.5778192 ,
                          -0.97491834,  0.41164221, -0.86091034]])

# Predict using the loaded K-Nearest Neighbors model
m1_knn = model_knn_loaded.predict(features_knn)
print("Predicted Price using K-Nearest Neighbors:", m1_knn)